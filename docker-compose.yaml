services:

  zookeeper:
    image: bitnami/zookeeper
    ports: ["2181:2181"]
    environment:
      ALLOW_ANONYMOUS_LOGIN: yes

  kafka:
    image: bitnami/kafka
    ports: ["9092:9092"]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
    depends_on: [zookeeper]

  kafka_init:
    image: confluentinc/cp-kafka
    depends_on: [kafka]
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic weather_data --replication-factor 1 --partitions 1
      echo 'Existing Kafka topics:'
      kafka-topics --bootstrap-server kafka:9092 --list
      "

  kafka_weather_producer:
    build: data-ingestion/.
    depends_on: [kafka]
    env_file: [.env]
    environment:
      WEATHER_API_KEY: ${WEATHER_API_KEY}
      CITY_NAME: Warsaw
      KAFKA_TOPIC: ${KAFKA_TOPIC}

  spark:
    build: spark/.
    environment:
      SPARK_MODE: master
    volumes: [
      ./spark/spark-jobs:/opt/spark-jobs,
      ./hive/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
    ]
    ports: ["7077:7077", "8080:8080"]

  spark-worker:
    image: bitnami/spark
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark:7077
    depends_on: [spark]

  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    environment:
      CLUSTER_NAME: test
      CORE_CONF_fs_defaultFS: hdfs://hdfs-namenode:8020
    volumes: [hdfs-namenode:/hadoop/dfs/name]
    ports: ["9000:9000", "8020:8020"]

  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    environment:
      CLUSTER_NAME: test
      CORE_CONF_fs_defaultFS: hdfs://hdfs-namenode:8020
    volumes: [hdfs-datanode:/hadoop/dfs/data]
    depends_on: [hdfs-namenode]

  postgres:
    image: postgres:12
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    ports: ["5432:5432"]
    volumes: [pgdata:/var/lib/postgresql/data]

  hive-metastore-init:
    image: bde2020/hive:2.3.2-postgresql-metastore
    environment:
      HIVE_METASTORE_DB_TYPE: postgres
      HIVE_METASTORE_DB_URI: "jdbc:postgresql://postgres:5432/metastore"
      HIVE_METASTORE_DB_USER: hive
      HIVE_METASTORE_DB_PASSWORD: hive
      CORE_CONF_fs_defaultFS: hdfs://hdfs-namenode:8020
    entrypoint: /bin/bash -c "export HIVE_CONF_DIR=/opt/hive/conf && /opt/hive/bin/schematool -initSchema -dbType postgres"
    volumes: [./hive/hive-site.xml:/opt/hive/conf/hive-site.xml]
    depends_on:
      [hdfs-namenode, hdfs-datanode, postgres]

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    environment:
      HIVE_METASTORE_DB_TYPE: postgres
      HIVE_METASTORE_DB_URI: "jdbc:postgresql://postgres:5432/metastore"
      HIVE_METASTORE_DB_USER: hive
      HIVE_METASTORE_DB_PASSWORD: hive
      HIVE_METASTORE_PORT: 9083
      CORE_CONF_fs_defaultFS: hdfs://hdfs-namenode:8020
    command: /opt/hive/bin/hive --service metastore
    ports: ["9083:9083"]
    volumes: [./hive/hive-site.xml:/opt/hive/conf/hive-site.xml]
    depends_on: [hive-metastore-init]

  hive:
    image: bde2020/hive:2.3.2-postgresql-metastore
    environment:
      HIVE_METASTORE_URI: thrift://hive-metastore:9083
      HIVE_CONF_hive_exec_scratchdir: /tmp/hive
      HIVE_CONF_hive_log_dir: /tmp/hive
      CORE_CONF_fs_defaultFS: hdfs://hdfs-namenode:8020
    ports: ["10000:10000"]
    volumes: [./hive/hive-site.xml:/opt/hive/conf/hive-site.xml]
    depends_on: [hive-metastore]

  hive-init:
    image: bde2020/hive:2.3.2-postgresql-metastore
    environment:
      HIVE_METASTORE_URI: thrift://hive-metastore:9083
      HIVE_CONF_hive_exec_scratchdir: /tmp/hive
      HIVE_CONF_hive_log_dir: /tmp/hive
      CORE_CONF_fs_defaultFS: hdfs://hdfs-namenode:8020
    volumes: [./hive/hive-init.sh:/docker-entrypoint-initdb.d/hive-init.sh, ./hive/hive-site.xml:/opt/hive/conf/hive-site.xml]
    depends_on: [hive]
    entrypoint: ["/docker-entrypoint-initdb.d/hive-init.sh"]

volumes:
  hdfs-namenode:
    external: false
  hdfs-datanode:
    external: false
  pgdata:
    external: false